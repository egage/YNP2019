---
title: "Well Data - Data Loggers"
output:
  html_document: 
    fig_caption: yes
    fig_height: 8
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
# opts_knit$set(root.dir=normalizePath('../')) # required if Rmd is nested below the project directory

```

```{r, eval=TRUE, echo = FALSE}
library(dplyr)
files.csv <- fs::dir_ls("./data/raw/WinSituData", recurse = TRUE, glob = "*.csv") %>% 
  length()
```

**Updated: `r format(Sys.time(), '%Y %B %d')`**

## Introduction

This document and code provides scrape data from the scattered well logger files, combines into a single analyzable file. In addition to the screening and merging of over `r files.csv`  files, the code conducts basic exploratory analyses of well logger data, primarily for further QA/QC.

The inherited state of organization for the data logger files was a mess. There are dozens of partially overlapping and inconsistently named files to munge. The DCC data (the data that was added to the DCC library repository) are assumed to be reliable. 
The code crapes data from raw logger files discovered while searching for data in the different shared dropbox/drive folders and DK hard drives. 

DCC were processed to yield relative water table elevations by D. Kotter, while newer files need correction for stickup/hang depth. These data are presumed to exist somewhere and are needed to evaluate possible errors in barometric corrections used from the sole barotroll (Tower) found for the post-DCC period. As indicated, these data are raw and need connection to field measurements (e.g., hang height, well stickup). Combined data retain file_name and serial number as attributes to aid in the necessary detective work using field notes.

Loggers vary in their start and logging interval, so to synchronize, data were reduced to a daily step for export.

```{r,echo=FALSE}
# library(here)
# here()
# install.packages("bindrcpp")
suppressPackageStartupMessages(library(tidyverse))
library(fs)
suppressPackageStartupMessages(library(sf))
# library(raster)
suppressPackageStartupMessages(library(janitor))
suppressPackageStartupMessages(library(readxl))
# library(glue)
suppressPackageStartupMessages(library(mapview))
# library(ggmap)
# library(ggrepel)
suppressPackageStartupMessages(library(viridis))
library(ggExtra)
library(DT)
library(kableExtra)
suppressPackageStartupMessages(library(lubridate))
library(anytime)
suppressPackageStartupMessages(library(compare))
suppressPackageStartupMessages(library(skimr)) ## some useful functions
suppressPackageStartupMessages(library(dataMaid))
library(gt)
```


# 2012-2015 (DCC Collection)

### Data import and initial inspection 
```{r}
# import of tsv 
dcc.raw <- readr::read_tsv("data/NSF_DataArchive20180208/Well_Logger_Data_2012-2015/Yell_Well_Data_Logger2012_2015.txt")

dcc <- dcc.raw %>% 
  clean_names() %>% 
  tibble::rownames_to_column(var = "RecordID") 

```

There are various issues apparent on inspection.  For example:

> There are corrupt data in the DCC file. No useable time stamps -- everything is characterer in date format.     
> "date_and_time" is not formatted as dttm, just a simple date. It's redundant with 'date'
> The 'time' column is not, in fact, time data.
May wish to find the orginal logger data. This may work for daily aggregated data. Otherwise, not sure what to do with the 6 per-day records. 
> The 'second's column ranges from `r min(dcc$seconds)` to `r max(dcc$seconds)`. I guess this is a cumulative time elapsed, but from when to when? I'm still of the mind that daily aggregates are the way to go forward...

```{r}
## data type changes
dcc <- dcc %>% 
  mutate(date = mdy(date))

## change case to ease merging with non-DCC data
dcc <- dcc %>% 
  mutate(site = tolower(site)) %>% 
  mutate(wat_id2 = tolower(wat_id2)) %>% 
  mutate(year = as.integer(year)) %>% 
  rename(serial_number = serial_num)

```

```{r}
## make lower case
dcc <- dcc %>% 
  mutate_if(.predicate = is.character,.funs = tolower)

## modify "site" to be consistent with usage elsewhere
dcc <- dcc %>%
  rename(site.dcc = site) %>% 
  mutate(site = case_when(
    grepl("eb1", site.dcc) ~ "eb1",
    grepl("eb2", site.dcc) ~ "eb2",
    grepl("elk1", site.dcc) ~ "elk1",
    grepl("elk4", site.dcc) ~ "elk4",
    grepl("elkdx", site.dcc) ~ "elk",
    grepl("elkcc", site.dcc) ~ "elk",
    grepl("wbcc", site.dcc) ~ "wb",
    grepl("wbdx", site.dcc) ~ "wb",
    TRUE ~ site.dcc))

## add "plot" consistent with usage elsewhere
dcc <- dcc %>% 
  mutate(plot = case_when(
    grepl("cc", site.dcc) ~ "cc",
    grepl("dx", site.dcc) ~ "dx",
    grepl("dc", site.dcc) ~ "dc",
    grepl("cx", site.dcc) ~ "cx",
    TRUE ~ "obs")) %>% 
  mutate(site2 = paste0(site,"-",plot)) %>% 
  select(-site.dcc)
```

```{r}
# create a lu
lu.serialNum.watId2 <- dcc %>%
  distinct(serial_number, wat_id2)

```


```{r, eval=FALSE}
visdat::vis_dat(dcc, warn_large_data = FALSE)

dcc %>% 
  distinct(time)
```

```{r}
## what are the distinct wells?
# dcc %>% 
#   distinct(site_logger) %>% 
#   gt() %>% 
#   tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

# note there are 19 distinct "site_logger" values and 19 distinct "serial_num"
dcc %>% 
  distinct(site, serial_number) %>% 
  gt() %>% 
  tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

```


```{r}
### summarize at a daily time step
dcc.dly.mean <- dcc %>% 
  group_by(site, plot, site2, wat_id2, serial_number, date) %>% 
  summarise(mean.dly.abslevel = mean(abslevel,na.rm = TRUE), mean.water_column_m = mean(water_column_m, na.rm = TRUE), psi_logger = mean(press_psi, na.rm=TRUE),psi_baro = mean(bar_press_psi, na.rm=TRUE))

dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(plot = case_when(str_detect(site2, "cc") ~ "cc",
                          str_detect(site2, "dc") ~ "dc",
                          str_detect(site2, "dx") ~ "dx",
                          str_detect(site2, "cx") ~ "cx",
                          TRUE ~ "obs"))

## add in year and mob=nth columns
dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(yr = lubridate::year(date)) %>% 
  mutate(month = lubridate::month(date)) %>% 
  mutate(doy = lubridate::yday(date)) %>% 
  mutate(doy.d = format(date, format="%m-%d"))

```

```{r, eval=FALSE}
## some plots for QA
dcc.dly.mean %>% 
  mutate(mean.water_column_m = mean.water_column_m*-1) %>%
  # distinct(plot)
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.water_column_m)) +
  geom_line(aes(color=serial_number)) +
  geom_hline(yintercept = 0, color = "red", lty = "dashed") +
  labs(title = "DCC: Mean daily 'water_column_m' values") +
  facet_wrap(~site, ncol=2) +
  theme_minimal() +
  labs(x = "DOY", y = "mean.dly.abslevel", caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

```

```{r}
dcc.dly.mean %>% 
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.dly.abslevel)) +
  geom_line(color="blue") +
  labs(title = "DCC: Mean daily 'abslevel' values") +
  facet_wrap(~serial_number, scales = "free_y") +
  theme_minimal() +
  labs(x = "Date", y = "Water table elevation (m)",caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

# ggsave("waterlevel_loggers_exp.png", width = 8.5, height = 6)
```


```{r, fig.height=15}
## plot daily traces
dcc.dly.mean %>% # names()
  filter(plot != "obs") %>%
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  # ggplot(aes(x=doy,y=mean.water_column_m)) +
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  # labs(title = "Mean daily 'water_column_m' values") +
  facet_wrap(~wat_id2, scale = 'free_y', ncol = 3) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```

DCC daily traces

```{r, fig.height=15}
## plot daily traces

dcc.dly.mean %>%
  filter(plot != "obs") %>%
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  facet_wrap(~site2, scale = 'free_y', ncol = 2) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```


```{r, eval=FALSE}
### Export munged DCC data
dcc.dly.mean %>% 
  write_csv("./data/processed/data_loggers/dcc_logger_20201205.csv")

```

# Non-DCC 2016-2019 files

General strategy is to work backwards from 2019 as there innumerable 'appended' files. Maybe these capture data from earlier periods?


```{r, echo=FALSE, eval=FALSE}
## File inventory:  *.wsl
## find WSL files
files_all_wsl <- fs::dir_ls("./data/raw/WinSituData", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

files_all_wsl %>% 
  select(-path_full) %>% 
  datatable(caption = "All *.wsl files")
#  A total shit show, as expected.
```

There _many_ files. Some don't even seem to be from YELL (e.g. files from Sara Bisbing's AK work).

## Tower baro data

In this list is the Tower baro data. Pull this out. I believe it should be used for correcting all the different site data? There seem to be no other baro files....

Source file: TowerBarow_2011+_2019-09-10_11-01-36-258.csv
Based on the date range, this may extend to the full time series?
```{r}
tow.baro <- read_csv("data/raw/WinSituData/Logger_Data_2019/TowerBarow_2011+_2019-09-10_11-01-36-258.csv", skip=84) %>% 
  clean_names() %>% 
  select(-x5) %>%
  mutate(date = anytime::anydate(date_and_time))

## the start of the trace looks a little dodgy, trim
tow.baro <- tow.baro %>% 
  filter(date > 2011-10-02)

## extract date and calc mean daily pressure (entered in mm Hg) and temp
tow.baro.dly <- tow.baro %>% 
  group_by(date) %>% 
  dplyr::summarize(mmHg_mean = mean(pressure_mm_hg, na.rm = TRUE), temp_c_mean = mean(temperature_c)) %>% 
  mutate(yr = year(date))

# convert the mm Hg to psi (units the loggers are in)
tow.baro.dly <- tow.baro.dly %>% 
  select(-yr) %>%
  mutate(baroTow_psi = 0.0193368*mmHg_mean) %>% 
  rename(baroTow_tempC = temp_c_mean) %>% 
  rename(baroTow_mmHg = mmHg_mean)

```

```{r}
## plot baro data
tow.baro.dly %>% 
  # filter(mmHg.mean < 620) %>% 
  ggplot(aes(date, baroTow_psi)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily psi", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

tow.baro.dly %>% 
  ggplot(aes(date, baroTow_tempC)) +
  geom_line(color = 'red') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily temp_c", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

Temps below -30 C seem highly questionable...


## 2019 Data

### Data import and initial inspection 

```{r, echo=FALSE, eval=FALSE}
## find WSL files
files2019wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))
```

```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2019csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
files2019csv <- files2019csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2019csv <- files2019csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

```

For for 2019, there are `r files2019csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. These appear to mainly be 
raw export files from WinSitu. Pending on the particular notes entered in the logger, data starts at a different row in the *.csv, complicating parsing. Data are uncorrected. The only baro data seems to be from Tower and is used to process the data.

```{r}
## table of distinct file names
files2019csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2019 directory")

# Pull out what appears to be the only baro file
```

```{r}
## exclude the baro from the remaining logger files
files2019csv <- files2019csv %>% 
  filter(file_name != "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

```{r}
## all of the csv exports lack depth; uncorrected.

## add some header info
files2019csv <- files2019csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

```

```{r}
## Munge: extract out info from file header
files2019csv <- files2019csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  mutate(site_hdr = site)
```


Note: 'site_hdr' refers to the 'site' attribute recorded in the export file (i.e., it was entered in the logger).

```{r, eval=FALSE}
files2019csv %>%
  pluck(5,3) %>% 
  # head() %>% 
  View()

```

> Attribute with the serial number and site from the header. Note: the site is as exists in the logger. Does not match format used elswhere, so more munging...

```{r}
## extract out the actual data
logger_raw2019 <- files2019csv %>%
  unnest(import)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header
logger_raw2019 <- logger_raw2019 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```

> Distill to daily time step

```{r}
## create daily
logger_raw2019.dly <- logger_raw2019 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2019.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```

```{r}
## join in the baro

logger_raw2019.dly.baro <- left_join(logger_raw2019.dly, tow.baro.dly, by = "date") 

```

```{r, eval=FALSE}
logger_raw2019.dly.baro %>% 
  visdat::vis_dat()

logger_raw2019.dly.baro %>% 
  View()

```


## 2018 Data 

### Data import and initial inspection 


Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
-	**Data file labeled Slough Creek is WBCC.** Pressure transducer was re-labeled for Slough Creek in 2017 but never deployed. Redeployed in spring to WBCC and not renamed. 
-	Crystal well was removed in either May or June 2018 and replaced in July 2018. Check timing of removal in May/June. Logger was likely removed from the well, not paused, and returned in July after a reset. 
-	All data need to be post-processed. All stick-up, total depth, and hanging distances are recorded in the well measurement field note books, which will eventually be copied to a spreadsheet.  


```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2018csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.csv")

## find WSL files
files2018wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

## make a tibble, rename
files2018csv <- files2018csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2018csv <- files2018csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

# files2018csv %>% 
#   distinct(file_name) %>% datatable()
```

For for 2018 alone, there are `r files2018csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconstienly structured. 

```{r}
## table of distinct file names
files2018csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2018 directory")

```

```{r}
## all of the csv exports lack depth; uncorrected.
files2018csv <- files2018csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 40))

## Munge: extract out info from file header
files2018csv <- files2018csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  rename(site_hdr = site) 

## remove baro
files2018csv <- files2018csv %>%
  filter(file_name != "2011+_2018-08-22_08-59-44-597.csv") # remove the tower baro data. I've previously pulled that out (and it's in mm Hg)

```

```{r, eval=FALSE}
files2018csv %>%
  pluck(5,2) %>% 
  head() %>% View()

files2018csv %>% 
  pluck(4,2) %>% datatable()

## get names
files2018csv %>% 
  mutate(names = map(import, names)) %>% 
  unnest(names) %>% 
  datatable()

```

```{r, eval=FALSE}
## explore parsing
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv", skip = 69)
t
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv")
t
```

```{r}
## extract out the actual data
logger_raw2018 <- files2018csv %>%
  unnest(import)

# logger_raw2018 %>% 
#   distinct(file_name)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header. Take a long time to process...
logger_raw2018 <- logger_raw2018 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```


> Distill to daily time step

```{r}
## create daily
logger_raw2018.dly <- logger_raw2018 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2018.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```


```{r}
## join in the baro

logger_raw2018.dly.baro <- left_join(logger_raw2018.dly, tow.baro.dly, by = "date") 

```

```{r, eval=FALSE}
logger_raw2018.dly.baro %>% 
  visdat::vis_dat()

logger_raw2018.dly.baro %>% 
  View()

```

```{r}
# combine and distill down the 2018 and 2019 batches
logger_raw2018_19.dly.baro <- bind_rows(logger_raw2018.dly.baro, logger_raw2019.dly.baro) %>% 
  distinct()

## add yr
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(yr = as.integer(year(date)))
```

```{r}
## qa
# logger_raw2018_19.dly.baro %>% 
#   head()

logger_raw2018_19.dly.baro %>% 
  tabyl(site_hdr, yr) %>% 
  datatable(caption = "Count of records by yr and site_hdr")

logger_raw2018_19.dly.baro %>% 
  tabyl(serial_number, yr) %>% 
  datatable(caption = "Count of records by yr and serial number")

```

```{r}
## munge site
logger_raw2018_19.dly.baro %>% 
  distinct(site_hdr)

## do my best to guess site and plot from the site_hdr field derived from logger header
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(site2 = case_when(site_hdr == "WestBlacktail_exp dx" ~ "wb-dx",
                           site_hdr == "Crystal_Well" ~ "crystal-obs",
                           site_hdr == "Crystal3" ~ "crystal-obs",
                           site_hdr == "EastBlacktail1_exp cc" ~ "eb1-cc",
                           site_hdr == "EastBlacktail1_exp dx" ~ "eb1-dx",
                           site_hdr == "EastBlacktail2_obs" ~ "eb2-obs",
                           site_hdr == "EastBlacktail2_exp cc" ~ "eb2-cc",
                           site_hdr == "EastBlacktail2_exp dc" ~ "eb2-dc",
                           site_hdr == "EastBlacktail2_exp dx" ~ "eb2-dx",
                           site_hdr == "Elk1_obs" ~ "elk1-obs",
                           site_hdr == "Elk4_obs" ~ "elk4-obs",
                           site_hdr == "Elk5" ~ "elk5-obs",
                           site_hdr == "Elk_exp cc" ~ "elk-cc",
                           site_hdr == "Elk_exp dx" ~ "elk-dx",
                           site_hdr == "LowerBlacktail2_obs" ~ "lb2-obs",
                           site_hdr == "LB4_WELL" ~ "lb4-obs",
                           site_hdr == "Oxbow_obs" ~ "oxbow-obs",
                           site_hdr == "Slough" ~ "wb-cc",
                           site_hdr == "WB4" ~ "wb4-obs",
                           site_hdr == "LostCreek_obs" ~ "lostc-obs",
                           TRUE ~ site_hdr)) %>% 
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

# Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
# -	**Data file labeled Slough Creek is WBCC.
```

```{r}
logger_raw2018_19.dly.baro %>% 
  select(serial_number,site_hdr,site2,site,plot) %>% 
  distinct() %>% 
  gt() %>% 
  tab_header(title = "site info attributed from logger site (site_hdr)", subtitle = "LEWIS please check")

## write to csv to share with LEWIS
# logger_raw2018_19.dly.baro %>% 
#   select(serial_number,site_hdr,site2,site,plot) %>% 
#   distinct() %>%
#   write_csv("./data/processed/logger_site_attribution_4QC.csv")

```

```{r}
## first attempt at baro correction
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) 

```

```{r}
logger_raw2018_19.dly.baro %>% 
  summarytools::descr(head_cm) %>% 
  summarytools::tb()

logger_raw2018_19.dly.baro %>% 
  ggplot(aes(date,head_cm)) +
  geom_line(aes(color = plot)) +
  facet_wrap(~site) +
  labs(title = "QA plot", subtitle = "logger_psi - baro_psi converted to cm of head")

```

I don't know where missing site data should be. Also, info on the specific well id and location is lacking, so not able to compare to manual well measurements. No notes on stick up or hang depth needed to further process the data. 

## Misc other files 

There are several additional directories of files. Scraping these for any data >2016. 
Presume earlier data folded up in the DCC?

### Data import and initial inspection 

```{r}
# the lack of data management on this project is ridiculous.

# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
filesOther_csv <- fs::dir_ls("./data/raw/WinSituData/Exported Data", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
filesOther_csv <- filesOther_csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
filesOther_csv <- filesOther_csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

filesOther_csv %>%
  distinct(file_name) %>% datatable()
```

In this directory alone, there are `r filesOther_csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. 

```{r}
## table of distinct file names
filesOther_csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 'Exported Data' folder", subtitle = "A bunch of S$#T. All older than DCC. There are even files from Sara Bisbing's AK project.")

```

```{r}
## there are baromerged files. These have different columns

## extract the baromerged
filesOther_csv.baromrg <- filesOther_csv %>% 
  filter(str_detect(file_name, "BaroMerge"))


```

```{r}
## exclude the baromerged
filesOther_csv <- filesOther_csv %>% 
  filter(!str_detect(file_name, "BaroMerge"))         
## add some header info
filesOther_csv <- filesOther_csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

## Munge: extract out info from file header
filesOther_csv <- filesOther_csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() 
```

> Attribute with the serial number from the header. 

```{r}
## extract out the actual data
logger_rawOther <- filesOther_csv %>%
  unnest(import)

```

```{r, cache=FALSE}
# select and attribute with the serial number and site from the header
logger_rawOther <- logger_rawOther %>%
  select(file_name,date_time, psi, temp_c, serial_number) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))

```

> Distill to daily time step

```{r}
## create daily
logger_rawOth.dly <- logger_rawOther %>% 
  group_by(file_name,date, serial_number) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  gt() %>% 
  tab_header(title = "Most files in this batch do not have any 2016+ data")

```

>Most this seems useless. It should be in the DCC if max year <=2015

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  filter(max.yr >= 2016) %>% 
  gt() %>% 
  tab_header(title = "Only three files in this batch have any 2016+ data")

```



```{r}

rawOth.16.logger <- logger_rawOth.dly %>% 
  filter(psi.dly.mean < 100) %>% # baro data in mmHg filtered out here
  filter(date > '2016-01-01')

```

```{r, eval=FALSE}
rawOth.16.logger %>% 
  distinct(file_name) %>% 
  gt() %>% 
  tab_header(title = "The only new data produced from this directory scrape")

```


```{r}
# rawOth.16.logger %>% 
#   names()

rawOth.16.logger <- left_join(rawOth.16.logger, tow.baro.dly) 

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(site2 = case_when(grepl("Elk5", file_name, ignore.case = TRUE) ~ "elk5-obs",
                                     TRUE ~ file_name))

rawOth.16.logger <- rawOth.16.logger %>%
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) %>% 
  mutate(yr = as.integer(year(date)))

```

```{r}
## bind with the 2018 and 2019 scrape
logger.all <- bind_rows(rawOth.16.logger, logger_raw2018_19.dly.baro)

# logger.all %>% 
#   mutate(yr = as.integer(year(date))) %>% 
#   visdat::vis_dat(warn_large_data = FALSE)

```

All that crap to net ~500 records in 2016 for one site!

> Join in wat_id2 to the non-DCC scrape 
> ASSUMPTION: serial number stay with wells. The well number associated with a given serial number in the DCC is used to attribute the non-DCC scrape. 

```{r}
## Filter to 2016 +
logger.all <- logger.all %>%
  filter(yr >=2016) 
```


```{r}
## join in wat_id2 to the non-DCC scrape 
lu.serialNum.watId2 <- lu.serialNum.watId2 %>% 
  mutate(serial_number = as.character(serial_number))

logger.all <- dplyr::left_join(logger.all,lu.serialNum.watId2, by="serial_number")

```

There are multiple files with serial numbers not present in the DCC. **Are these new loggers?**


```{r}
## there are data not joining to dcc on serial number
logger.all %>% 
  filter(is.na(wat_id2)) %>%
  distinct(file_name) %>% 
  gt() %>% 
  tab_header(title = "Files lacking coresponding serial number in DCC")

```

```{r}
## Filter to 2016 +
logger.all <- logger.all %>%
  filter(yr >=2016) 

## prep for row bind with DCC

dcc.dly.mean <- dcc.dly.mean %>% 
  ungroup() %>% 
  mutate(file_name = "DCC") %>% 
  mutate(serial_number = as.character(serial_number)) %>% 
  mutate(head_cm = mean.water_column_m*100) %>% 
  mutate(serial_number = as.character(serial_number)) %>% 
  select(-c(month, doy, doy.d, mean.water_column_m)) 
logger.all <- logger.all %>% 
  select(-c(baroTow_mmHg,baroTow_tempC)) %>% 
  rename(psi_logger = psi.dly.mean) %>% 
  rename(psi_baro = baroTow_psi) %>% 
  select(-c(site_hdr))

logger.all %>%
  glimpse()

# compare_df_cols_same(logger.all, dcc.dly.mean)

```

```{r}
### BIND DCC
logger.all <- logger.all %>% 
  bind_rows(.,dcc.dly.mean) %>% 
  distinct()
```


```{r}
## add in dates
logger.all <- logger.all %>% 
  mutate(yr = lubridate::year(date)) %>% 
  mutate(month = lubridate::month(date)) %>% 
  mutate(doy = lubridate::yday(date)) %>% 
  mutate(doy.d = format(date, format="%m-%d"))

```



# Exploratory Analyses and QA

```{r, eval=FALSE}
logger.all %>% 
  visdat::vis_dat(warn_large_data = FALSE) +
  labs(caption = "Combined DCC and other logger data")
```


```{r, fig.width=7.5, fig.height=8}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")

```

> Interactive time series plot - Exp sites only

```{r, fig.width=7.5, fig.height=8}
qa.pl.exp <- logger.all %>%
  filter(plot != "obs") %>% 
  ggplot(aes(date, head_cm)) +
  geom_point(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free_y", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")
plotly::ggplotly(qa.pl.exp)
```

> Interactive time series plot - Obs sites only

```{r, fig.width=7.5, fig.height=8}
qa.pl.obs <- logger.all %>%
  filter(plot == "obs") %>% 
  ggplot(aes(date, head_cm)) +
  geom_point(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")
plotly::ggplotly(qa.pl.obs)
```

```{r, fig.width=7.5, fig.height=10}
logger.all %>%
  mutate(doy = yday(date)) %>%
  mutate(yr = as.factor(yr)) %>% 
  ggplot(aes(doy, head_cm)) +
  geom_line(aes(color = yr)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~serial_number, scales = "free", ncol = 2) +
  labs(title = "Time series -- all logger data", x="", y="Head (cm)", caption = "logger.all+DCC")

```


```{r, fig.width=7, fig.height=6.5}
logger.all %>%
  group_by(serial_number, site, plot, yr) %>% 
  tally() %>% 
  ggplot(aes(yr, serial_number)) +
  theme_minimal() +
  geom_tile(aes(fill = n), color = "white") +
  facet_wrap(~site, scales = "free", ncol=2) +
  labs(fill = "n records", y = "Serial number", caption = "logger.all+DCC")

```

```{r, fig.width=7, fig.height=10}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~yr, scales = "free", ncol=2) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(caption = "combined scrape of all raw files + DCC") +
  facet_wrap(~site, ncol=3)

```

Some take homes:

> There are gaps in data

> (Some) out of range values likely linked to when loggers deployed/read

Calculating a z score for logger serial numbers and years. This is a pretty roug way to process.

```{r}
logger.all.clean <- logger.all %>% 
  group_by(serial_number,yr) %>% 
  mutate(z_score.bySerial = (psi_logger - mean(psi_logger)) / sd(psi_logger)) %>% 
  ungroup() %>% 
  mutate(flag.zscore = case_when(abs(z_score.bySerial) > 2 ~ "z>2")
         )

```

```{r}
logger.all.clean <- logger.all.clean %>% 
  select(-c(tempC.dly.mean, psi.dif, mean.dly.abslevel)) 


logger.all.clean %>% 
  visdat::vis_dat(warn_large_data = FALSE)

logger.all.clean %>%
  filter(is.na(wat_id2)) %>% 
  distinct(site)

```


```{r, eval=FALSE}
logger.all.clean %>% 
  summarytools::descr(z_score.bySerial)

```

> z scores (by serial number and yr): all data

```{r, fig.width=8, fig.height=8}
## all data
logger.all.clean %>% 
  ggplot(aes(z_score.bySerial,serial_number)) +
  geom_boxplot() +
  facet_wrap(~yr) +
  theme_minimal() +
  labs(caption = "z scores (by serial number and yr): all records")

```

> z scores (by serial number and yr): |z-score| < 2
 
```{r, fig.width=8, fig.height=10}
logger.all.clean %>%
  filter(is.na(flag.zscore)) %>% 
  ggplot(aes(z_score.bySerial,serial_number)) +
  geom_boxplot() +
  facet_wrap(~yr) +
  theme_minimal() +
  labs(caption = "z scores (by serial number and yr): |z-score| < 2")

```

```{r, fig.width=8, fig.height=10}
logger.all.clean %>%
  filter(is.na(flag.zscore)) %>% 
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~yr, scales = "free", ncol=2) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(caption = "combined scrape of all raw files + DCC; z-score < 2") +
  facet_wrap(~site, ncol=3)

```

```{r, eval=FALSE}
## Export for Further Analysis
logger.all.clean %>% 
  write_csv("./data/processed/data_loggers/visdat::vis_dat(warn_large_data = FALSE)combined_dataloggers_20201207.csv")
```


```{r}
## remove char from wat_id2 (e.g., "e5" > "5")
# library(stringr)
numextract <- function(string){ 
  stringr::str_extract(string, "\\-*\\d+\\.*\\d*")
} 

## rename/munge to bind in manual
logger.all.clean <- logger.all.clean %>%
  mutate(wid = numextract(wat_id2)) %>% 
  mutate(wid = as.character(wid))
## wat_id2 was carried in from the logger side...
```

Before joining manual and logger datasets, need to figure out what to do with the sites in the logger data set lacking a well id (wat_id2 in the logger data set)...

The following raw logger files/serial numbers lack a well ID. The well id (wat_id2) originated in the DCC. So these could have been new loggers or mis-attributed in the DCC. 
I'll see if I can deduce their identity, but field notes would help!

```{r}
logger.all.clean %>% 
  filter(is.na(wid)) %>% 
  distinct(wid, file_name, serial_number) %>% 
  gt() %>% 
  tab_header(title = "Logger files with NA for well id (wid)")

## create lu for anitjoin
missing.wid.lu <- logger.all.clean %>% 
  filter(is.na(wid)) %>% 
  distinct(wid, file_name, serial_number)

```

```{r}
### attributre missing wid for logger. I did some sluething.
# No wb4 in the manaual well measurements, so attributed the wid as "wb4". The other missing wid were possible to fix.

logger.all.clean <- logger.all.clean %>%
  mutate(wid = case_when(is.na(wid) & str_detect(site,"elk5") ~ "40",
                         is.na(wid) & str_detect(file_name,"WB4") ~ "wb4",
                         is.na(wid) & str_detect(file_name,"Slough") ~ "40",
         TRUE ~ wid))
# add fullid for join with manual
logger.all.clean <- logger.all.clean %>% 
  mutate(fullid = paste0(site,"-",plot,"-",wid))

# check
# logger.all.clean %>% 
#   filter(is.na(wid)) %>% 
#   distinct(wid, file_name, serial_number) %>% 
#   gt() %>% 
#   tab_header(title = "Logger files with NA for well id (wid)")
  
```

```{r, eval=FALSE}
## Export for Further Analysis
logger.all.clean %>% 
  write_csv("./data/processed/data_loggers/combined_dataloggers_20201208.csv")

```

# Integrate manual well measurements

The following code joins in manual well measurements, as processed from raw files in a different Rmd file. It's assumed loggers have fidelity to wells.  

Source: ./data/processed/manual_well_data_raw_20201202.csv
```{r}
# read in manual measurement
# This is the output of the manual well data Rmd that draws on the raw manual well data files
mw.01.19 <- read_csv("./data/processed/manual_well_data_raw_20201202.csv")

## type convert
mw.01.19 <- mw.01.19 %>% 
  mutate(wid = as.character(wid))

# cleave off some columns for join
mw.01.19 <- mw.01.19 %>% 
  select(-c(yr, doy, mo, mday, site, plot, wid, site2))

mw.01.19 <- mw.01.19 %>%
  mutate(rwte = rwte*-1)

mw.01.19 <- mw.01.19 %>%
  rename(mw_su_cm = su_cm, mw_dtw_cm = dtw_cm, mw_rwte = rwte, mw_flag = flag) %>%
  mutate(mw_flag = as.character(mw_flag))

```



```{r}
## join logger manual
log.man <- left_join(logger.all.clean, mw.01.19, by = c("date","fullid"))

log.man <- log.man %>% 
  select(-c(wat_id2, site_type))
```

```{r, eval=FALSE}
### qa
logger.all.clean %>% visdat::vis_dat(warn_large_data = FALSE) +
  labs(title = "logger.all.clean")
  

mw.01.19 %>% visdat::vis_dat(warn_large_data = FALSE) +
  labs(title = "mw.01.19")

mw.01.19 %>% filter(is.na(mw_dtw_cm))

log.man %>% visdat::vis_dat(warn_large_data = FALSE) +
  labs(title = "log.man")

```

```{r}
log.man.coincident <- log.man %>%
  filter(!is.na(mw_rwte))
```

```{r}


log.man.coincident %>% 
  select(file_name, yr, serial_number, site2, head_cm, mw_su_cm, mw_rwte) %>% 
  distinct() %>% datatable()

```

```{r}
# use SU from manual wells to calculate hang height and adjust head_cm from logger

# !!! fore records without su, assuming rwte + head = hang height

log.man.coincident <- log.man.coincident %>% 
  # mutate(hang.calc = mw_dtw_cm + head_cm) %>%
  mutate(hang.calc = case_when(
    !is.na(mw_dtw_cm) ~ (mw_dtw_cm + head_cm),
    is.na(mw_dtw_cm) ~ (mw_rwte + head_cm)))

log.man.coincident <- log.man.coincident %>% 
  mutate(rwte.calc = case_when(
    !is.na(mw_su_cm) ~ (hang.calc-head_cm-mw_su_cm),
    is.na(mw_su_cm) ~ (hang.calc-head_cm)))
  # mutate(rwte.calc = hang.calc-head_cm-mw_su_cm)

log.man.coincident %>% visdat::vis_dat(warn_large_data = FALSE) +
  labs(title = "log.man.coincident")

# #
# log.man.coincident %>% 
#   select(hang.calc, hang.calc) %>% 
#   datatable()

## distill
# log.man.hang.calc2join <- log.man.coincident %>%
#   distinct(yr, fullid, hang.calc)
log.man.hang.calc2join <- log.man.coincident %>%
  distinct(serial_number, hang.calc)
# log.man.hang.calc2join <- log.man.coincident %>%
#   distinct(fullid, hang.calc)

# dplyr::anti_join(logger.all.clean, log.man.hang.calc2join, by = "serial_number") %>% View()
```


The following table shows the coincident data in manual well measurements and loggers based on date and serial number

```{r}
log.man.coincident %>% 
  datatable()

```

## Estimate RWTE

Calculate hang height from head, su, and dtwt data. The code creates a flag field denoting whether a rwte calculated has info for hang height for that date or is estimated using past data and 0 su (applicable to SG?). **Hang heights from field forms (if they exist) should be brought it**


```{r}
log.man.cor <- left_join(log.man, log.man.hang.calc2join, by = c("serial_number"))

log.man.cor %>%
  filter(file_name != "DCC") %>% 
  filter(is.na(hang.calc)) %>% 
  tabyl(site2, yr)

## create flag denoting whether a rwte calculated has info for hang height. Otherwise estimated using past data and 0 su (applicable to SG?). **Hang heights from field forms (if they exist) should be brought it**

log.man.cor <- log.man.cor %>% 
  mutate(rwte.calc = hang.calc - head_cm) %>%
  mutate(flag_hanght = case_when(!is.na(rwte.calc) ~ "estimated")) %>% 
  mutate(rwte.calc = case_when(is.na(rwte.calc) & !is.na(mw_rwte) ~ mw_rwte,
    TRUE ~ rwte.calc))
  # mutate(rwte.calc = case_when(
  #   is.na(rwte.calc) & !is.na(mw_rwte) ~ mw_rwte,
  #   TRUE ~ rwte.calc
  # ))


```


## sg/well attribution
```{r}
## add lu from the "Well_Data.shp" file in the "geospatial" directory

lu_wat_id2 <- read_csv("./data/processed/lu_well_wat_id2_from_shape.csv")

logger.all %>% 
  distinct(wat_id2)

try <- left_join(logger.all,lu_wat_id2, by = "wat_id2")

try %>% 
  filter(is.na(type)) %>% 
  distinct(file_name, site, plot, wat_id2)

# I don't know for many of these which are sg vs wells
```

# >!


#>>
```{r}
## plotting functions
library(rlang)

# gg.tile <- function(mydf, myxcol, myycol, fill, mytitle, caption) {
#    ggplot2::ggplot(data = mydf, aes(x=reorder({{ myxcol }}, 
#       {{ myycol }}), y= {{ myycol }})) +
#     geom_tile(color = "black", fill="#0072B2") +
#     xlab("") +
#     ylab("") +
#     coord_flip() +
#     ggtitle(mytitle) +
#     labs(caption = caption) +
#     theme_minimal()   +
#     theme(plot.title=element_text(size=24))
# }

### 
gg.tile <- function(mydf, myxcol, myycol, fill, mytitle, caption) {
   ggplot2::ggplot(data = mydf, aes(x=reorder({{ myxcol }}, 
      {{ myycol }}), y= {{ myycol }})) +
    geom_tile(aes(fill = {{ fill }}), color = "grey") +
    xlab("") +
    ylab("") +
    ggtitle(mytitle) +
    labs(caption = caption) +
    theme_minimal()   +
    theme(plot.title=element_text(size=14))
}

## faceting version
gg.tile.facwr <- function(mydf, myxcol, myycol, fill, facetvar, mytitle, caption) {
   ggplot2::ggplot(data = mydf, aes(x=reorder({{ myxcol }}, 
      {{ myycol }}), y= {{ myycol }})) +
    geom_tile(aes(fill = {{ fill }}), color = "grey") +
    ggtitle(mytitle) +
    labs(caption = caption) +
    facet_wrap(~{{ facetvar }}) +
    theme_minimal()   +
    theme(plot.title=element_text(size=14))
}

```

```{r}
#### apply tile fun
## qaqc
log.man.cor %>% 
  filter(is.na(rwte.calc)) %>% 
  group_by(file_name, yr) %>% 
  tally() %>% 
  gg.tile(mydf = .,myxcol = file_name,myycol = yr, fill = n,mytitle = "",caption = "Missing rwte.calc records. combined logger/mw")

log.man.cor %>% 
  filter(is.na(rwte.calc)) %>% 
  group_by(serial_number, site, yr) %>% 
  tally() %>%
  gg.tile(mydf = .,myxcol = yr, myycol = serial_number, fill = n, mytitle = "",caption = "Missing rwte.calc records. combined logger/mw")

```

```{r}
log.man.cor %>% 
  # filter(is.na(rwte.calc)) %>%
  group_by(serial_number, site, yr) %>% 
  tally() %>%
  gg.tile(mydf = .,myxcol = yr, myycol = serial_number, fill = n, mytitle = "",caption = "All records. combined logger/mw")

```


```{r}
log.man.cor %>% 
  group_by(file_name, site, plot, yr) %>% 
  summarytools::descr(rwte.calc, stats = c("mean", "min", "max")) %>% 
  summarytools::tb() %>% 
  datatable(caption = "Summary stats: rwte.calc")
```

```{r}
log.man.cor %>% 
  visdat::vis_dat(warn_large_data = FALSE) +
  labs(caption = "Combined logger and manual well data")
```


```{r, eval=FALSE}
# qa
log.man %>% 
  filter(str_detect(file_name,"Elk5|WB4|Slough") | is.na(wid)) %>% 
  group_by(file_name, site,plot, serial_number, wid) %>% 
  tally()

```

```{r, eval=FALSE}
## qa
log.man %>% 
  filter(str_detect(site,"elk")) %>%
  select(site, plot, wid) %>% 
  distinct()

log.man %>% 
  filter(str_detect(site,"wb")) %>%
  select(site, plot, wid) %>% 
  distinct()

```


# Next steps

> Find information (if it exists) on hang depths, stickups, etc. These are back-calculated but is based on assumptions best checked. In addition, there are missing 

> Continue to filter out garbage 

> Make spatial by incorporating coordinates


