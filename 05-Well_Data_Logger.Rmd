---
title: "Well Data - Data Loggers"
output:
  html_document: 
    fig_caption: yes
    fig_height: 8
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = TRUE, message = FALSE, fig.align = 'center')
# opts_knit$set(root.dir=normalizePath('../')) # required if Rmd is nested below the project directory

```

**Updated: `r format(Sys.time(), '%Y %B %d')`**

## Introduction

This section provides basic QA/QC assessments for the well logger data.

Analyses aim to characterize basic data structure and identify potential issues, clean data as needed, then produce an archive-worthy output data.

Because they originate as separate source files, 2016-2018 data are brought in and examined separately. 

Data are cleaned to produce analyzable files for further analyses. 

```{r,echo=FALSE}
# library(here)
# here()
# install.packages("bindrcpp")
suppressPackageStartupMessages(library(tidyverse))
library(fs)
suppressPackageStartupMessages(library(sf))
# library(raster)
suppressPackageStartupMessages(library(janitor))
suppressPackageStartupMessages(library(readxl))
# library(glue)
suppressPackageStartupMessages(library(mapview))
# library(ggmap)
# library(ggrepel)
suppressPackageStartupMessages(library(viridis))
library(ggExtra)
library(DT)
library(kableExtra)
suppressPackageStartupMessages(library(lubridate))
library(anytime)
suppressPackageStartupMessages(library(compare))
suppressPackageStartupMessages(library(skimr)) ## some useful functions
suppressPackageStartupMessages(library(dataMaid))
library(gt)
```


## 2012-2015 (DCC Collection)

### Data import and initial inspection 
```{r}
# import of tsv 
dcc.raw <- readr::read_tsv("data/NSF_DataArchive20180208/Well_Logger_Data_2012-2015/Yell_Well_Data_Logger2012_2015.txt")

dcc <- dcc.raw %>% 
  clean_names() %>% 
  tibble::rownames_to_column(var = "RecordID") 

```

```{r, eval=FALSE}
## inspection
dcc %>% 
  glimpse()

```

There are various issues apparent on inspection.  For example:

> There are corrupt data in the DCC file. No useable time stamps -- everything is characterer in date format.     
> "date_and_time" is not formatted as dttm, just a simple date. It's redundant with 'date'
> The 'time' column is not, in fact, time data.
May wish to find the orginal logger data. This may work for daily aggregated data. Otherwise, not sure what to do with the 6 per-day records. 
> The 'second's column ranges from `r min(dcc$seconds)` to `r max(dcc$seconds)`. I guess this is a cumulative time elapsed, but from when to when? I'm still of the mind that daily aggregates are the way to go forward...

dcc %>% View()





and 'seconds' columns. 


```{r}
## data type changes
dcc <- dcc %>% 
  mutate(date = mdy(date))

## change case to ease merging with non-DCC data
dcc <- dcc %>% 
  mutate(site = tolower(site)) %>% 
  mutate(wat_id2 = tolower(wat_id2)) %>% 
  mutate(year = as.integer(year))


dcc %>%
  # distinct(date_and_time) %>% 
  head()
```

```{r}
visdat::vis_dat(dcc, warn_large_data = FALSE)

dcc %>% 
  distinct(time)
```




```{r}
## what are the distinct wells?
# dcc %>% 
#   distinct(site_logger) %>% 
#   gt() %>% 
#   tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

# note there are 19 distinct "site_logger" values and 19 distinct "serial_num"
dcc %>% 
  distinct(site, serial_num) %>% 
  gt() %>% 
  tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

```


```{r}

dcc <- dcc %>% 
  mutate_if(.predicate = is.character,.funs = tolower)

```


```{r}
dcc.dly.mean <- dcc %>% 
  group_by(site,site_logger, wat_id2, date) %>% 
  summarise(mean.dly.abslevel = mean(abslevel,na.rm = TRUE), mean.water_column_m = mean(water_column_m, na.rm = TRUE))


dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(plot = case_when(str_detect(site, "cc") ~ "cc",
                          str_detect(site, "dc") ~ "dc",
                          str_detect(site, "dx") ~ "dx",
                          str_detect(site, "cx") ~ "cx",
                          TRUE ~ "obs"))

## add in year and mob=nth columns
dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(yr = lubridate::year(date)) %>% 
  mutate(month = lubridate::month(date)) %>% 
  mutate(doy = lubridate::yday(date)) %>% 
  mutate(doy.d = format(date, format="%m-%d"))

```

```{r}
## some plots for QA
dcc.dly.mean %>% 
  mutate(mean.water_column_m = mean.water_column_m*-1) %>% 
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.water_column_m)) +
  # geom_line(aes(color=site_logger)) +
  geom_line(color="blue") +
  geom_hline(yintercept = 0, color = "red", lty = "dashed") +
  labs(title = "DCC: Mean daily 'water_column_m' values") +
  facet_wrap(~site, ncol=2) +
  theme_minimal() +
  labs(x = "DOY", y = "mean.dly.abslevel", caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

```

```{r}
dcc.dly.mean %>% 
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.dly.abslevel)) +
  geom_line(color="blue") +
  labs(title = "DCC: Mean daily 'abslevel' values") +
  facet_wrap(~site, scales = "free_y") +
  theme_minimal() +
  labs(x = "Date", y = "Water table elevation (m)",caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

# ggsave("waterlevel_loggers_exp.png", width = 8.5, height = 6)
```


```{r, fig.height=15}
## plot daily traces
dcc.dly.mean %>% # names()
  filter(plot != "obs") %>%
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  # ggplot(aes(x=doy,y=mean.water_column_m)) +
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  # labs(title = "Mean daily 'water_column_m' values") +
  facet_wrap(~site_logger, scale = 'free_y', ncol = 3) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```

```{r, fig.height=15}
## plot daily traces

dcc.dly.mean %>% # names()
  filter(plot != "obs") %>%
  filter(yr != "2012") %>% 
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  # ggplot(aes(x=doy,y=mean.water_column_m)) +
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  # labs(title = "Mean daily 'water_column_m' values") +
  facet_wrap(~site_logger, scale = 'free_y', ncol = 3) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```

# 2016-2019 files

General strategy is to work backwards from 2019 as there innumerable 'appended' files. Maybe these capture data from earlier periods?

## File inventory:  *.wsl
```{r}
## find WSL files
files_all_wsl <- fs::dir_ls("./data/raw/WinSituData", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

```

There _many_ files. Some don't even seem to be from YELL (e.g. files from Sara Bisbing's AK work).
```{r}
files_all_wsl %>% 
  select(-path_full) %>% 
  datatable(caption = "All *.wsl files")
#  A total shit show, as expected.
```

## Pull out Tower baro data

In this list is the Tower baro data. Pull this out. I believe it should be used for correcting all the different site data? There seem to be no other baro files....

Source file: TowerBarow_2011+_2019-09-10_11-01-36-258.csv
Based on the date range, this may extend to the full time series?
```{r}
tow.baro <- read_csv("data/raw/WinSituData/Logger_Data_2019/TowerBarow_2011+_2019-09-10_11-01-36-258.csv", skip=84) %>% 
  clean_names() %>% 
  select(-x5) %>%
  mutate(date = anytime::anydate(date_and_time))

## the start of the trace looks a little dodgy, trim
tow.baro <- tow.baro %>% 
  filter(date > 2011-10-02)

## extract date and calc mean daily pressure (entered in mm Hg) and temp
tow.baro.dly <- tow.baro %>% 
  group_by(date) %>% 
  dplyr::summarize(mmHg_mean = mean(pressure_mm_hg, na.rm = TRUE), temp_c_mean = mean(temperature_c)) %>% 
  mutate(yr = year(date))

# convert the mm Hg to psi (units the loggers are in)
tow.baro.dly <- tow.baro.dly %>% 
  select(-yr) %>%
  mutate(baroTow_psi = 0.0193368*mmHg_mean) %>% 
  rename(baroTow_tempC = temp_c_mean) %>% 
  rename(baroTow_mmHg = mmHg_mean)

```

```{r}
## plot baro data
tow.baro.dly %>% 
  # filter(mmHg.mean < 620) %>% 
  ggplot(aes(date, baroTow_psi)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily psi", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

tow.baro.dly %>% 
  ggplot(aes(date, baroTow_tempC)) +
  geom_line(color = 'red') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily temp_c", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

Temps below -30 C seem highly questionable...


## 2019 Data

### Data import and initial inspection 

```{r, echo=FALSE, eval=FALSE}
## find WSL files
files2019wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))
```

```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2019csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
files2019csv <- files2019csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2019csv <- files2019csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

```

For for 2019, there are `r files2019csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. These appear to mainly be 
raw export files from WinSitu. Pending on the particular notes entered in the logger, data starts at a different row in the *.csv, complicating parsing. Data are uncorrected. The only baro data seems to be from Tower and is used to process the data.

```{r}
## table of distinct file names
files2019csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2019 directory")

# Pull out what appears to be the only baro file
```

```{r}
## exclude the baro from the remaining logger files
files2019csv <- files2019csv %>% 
  filter(file_name != "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

```{r}
## all of the csv exports lack depth; uncorrected.

## add some header info
files2019csv <- files2019csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

## Munge: extract out info from file header
files2019csv <- files2019csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  mutate(site_hdr = site)

```

Note: 'site_hdr' refers to the 'site' attribute recorded in the export file (i.e., it was entered in the logger).

```{r, eval=FALSE}
files2019csv %>%
  pluck(5,3) %>% 
  # head() %>% 
  View()

```

> Attribute with the serial number and site from the header. Note: the site is as exists in the logger. Does not match format used elswhere, so more munging...

```{r}
## extract out the actual data
logger_raw2019 <- files2019csv %>%
  unnest(import)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header
logger_raw2019 <- logger_raw2019 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```

> Distill to daily time step

```{r}
## create daily
logger_raw2019.dly <- logger_raw2019 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2019.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```

```{r}
## join in the baro

logger_raw2019.dly.baro <- left_join(logger_raw2019.dly, tow.baro.dly, by = "date") 

```

```{r}
logger_raw2019.dly.baro %>% 
  visdat::vis_dat()

logger_raw2019.dly.baro %>% 
  View()

```


## 2018 Data 

### Data import and initial inspection 


Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
-	**Data file labeled Slough Creek is WBCC.** Pressure transducer was re-labeled for Slough Creek in 2017 but never deployed. Redeployed in spring to WBCC and not renamed. 
-	Crystal well was removed in either May or June 2018 and replaced in July 2018. Check timing of removal in May/June. Logger was likely removed from the well, not paused, and returned in July after a reset. 
-	All data need to be post-processed. All stick-up, total depth, and hanging distances are recorded in the well measurement field note books, which will eventually be copied to a spreadsheet.  


```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2018csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.csv")

## find WSL files
files2018wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

## make a tibble, rename
files2018csv <- files2018csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2018csv <- files2018csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

# files2018csv %>% 
#   distinct(file_name) %>% datatable()
```

For for 2018 alone, there are `r files2018csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconstienly structured. 

```{r}
## table of distinct file names
files2018csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2018 directory")

```

```{r}
## all of the csv exports lack depth; uncorrected.
files2018csv <- files2018csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 40))

## Munge: extract out info from file header
files2018csv <- files2018csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  rename(site_hdr = site) 

## remove baro
files2018csv <- files2018csv %>%
  filter(file_name != "2011+_2018-08-22_08-59-44-597.csv") # remove the tower baro data. I've previously pulled that out (and it's in mm Hg)

```

```{r, eval=FALSE}
files2018csv %>%
  pluck(5,2) %>% 
  head() %>% View()

files2018csv %>% 
  pluck(4,2) %>% datatable()

## get names
files2018csv %>% 
  mutate(names = map(import, names)) %>% 
  unnest(names) %>% 
  datatable()

```

```{r, eval=FALSE}
## explore parsing
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv", skip = 69)
t
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv")
t
```

```{r}
## extract out the actual data
logger_raw2018 <- files2018csv %>%
  unnest(import)

# logger_raw2018 %>% 
#   distinct(file_name)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header. Take a long time to process...
logger_raw2018 <- logger_raw2018 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```


> Distill to daily time step

```{r}
## create daily
logger_raw2018.dly <- logger_raw2018 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2018.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```


```{r}
## join in the baro

logger_raw2018.dly.baro <- left_join(logger_raw2018.dly, tow.baro.dly, by = "date") 

```

```{r, eval=FALSE}
logger_raw2018.dly.baro %>% 
  visdat::vis_dat()

logger_raw2018.dly.baro %>% 
  View()

```

```{r}
# combine and distill down the 2018 and 2019 batches
logger_raw2018_19.dly.baro <- bind_rows(logger_raw2018.dly.baro, logger_raw2019.dly.baro) %>% 
  distinct()

## add yr
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(yr = as.integer(year(date)))
```

```{r}
## qa
# logger_raw2018_19.dly.baro %>% 
#   head()

logger_raw2018_19.dly.baro %>% 
  tabyl(site_hdr, yr) %>% 
  datatable(caption = "Count of records by yr and site_hdr")

logger_raw2018_19.dly.baro %>% 
  tabyl(serial_number, yr) %>% 
  datatable(caption = "Count of records by yr and serial number")

```

```{r}
## munge site
logger_raw2018_19.dly.baro %>% 
  distinct(site_hdr)

## do my best to guess site and plot from the site_hdr field derived from logger header
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(site2 = case_when(site_hdr == "WestBlacktail_exp dx" ~ "wb-dx",
                           site_hdr == "Crystal_Well" ~ "crystal-obs",
                           site_hdr == "Crystal3" ~ "crystal-obs",
                           site_hdr == "EastBlacktail1_exp cc" ~ "eb1-cc",
                           site_hdr == "EastBlacktail1_exp dx" ~ "eb1-dx",
                           site_hdr == "EastBlacktail2_obs" ~ "eb2-obs",
                           site_hdr == "EastBlacktail2_exp cc" ~ "eb2-cc",
                           site_hdr == "EastBlacktail2_exp dc" ~ "eb2-dc",
                           site_hdr == "EastBlacktail2_exp dx" ~ "eb2-dx",
                           site_hdr == "Elk1_obs" ~ "elk1-obs",
                           site_hdr == "Elk4_obs" ~ "elk4-obs",
                           site_hdr == "Elk5" ~ "elk5-obs",
                           site_hdr == "Elk_exp cc" ~ "elk-cc",
                           site_hdr == "Elk_exp dx" ~ "elk-dx",
                           site_hdr == "LowerBlacktail2_obs" ~ "lb2-obs",
                           site_hdr == "LB4_WELL" ~ "lb4-obs",
                           site_hdr == "Oxbow_obs" ~ "oxbow-obs",
                           site_hdr == "Slough" ~ "wb-cc",
                           site_hdr == "WB4" ~ "wb4-obs",
                           site_hdr == "LostCreek_obs" ~ "lostc-obs",
                           TRUE ~ site_hdr)) %>% 
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

# Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
# -	**Data file labeled Slough Creek is WBCC.
```

```{r}
logger_raw2018_19.dly.baro %>% 
  select(serial_number,site_hdr,site2,site,plot) %>% 
  distinct() %>% 
  gt() %>% 
  tab_header(title = "site info attributed from logger site (site_hdr)", subtitle = "LEWIS please check")

## write to csv to share with LEWIS
# logger_raw2018_19.dly.baro %>% 
#   select(serial_number,site_hdr,site2,site,plot) %>% 
#   distinct() %>%
#   write_csv("./data/processed/logger_site_attribution_4QC.csv")

```

```{r}
## first attempt at baro correction
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) 

```

```{r}
logger_raw2018_19.dly.baro %>% 
  summarytools::descr(head_cm) %>% 
  summarytools::tb()

logger_raw2018_19.dly.baro %>% 
  ggplot(aes(date,head_cm)) +
  geom_line(aes(color = plot)) +
  facet_wrap(~site) +
  labs(title = "QA plot", subtitle = "logger_psi - baro_psi converted to cm of head")

```

I don't know where missing site data should be. Also, info on the specific well id and location is lacking, so not able to compare to manual well measurements. No notes on stick up or hang depth needed to further process the data. 

## There are other directories of files 

There are several additional directories of files. Scraping these, but many are likely folded up in the DCC?

### Data import and initial inspection 

```{r}
# the lack of data management on this project is ridiculous.

# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
filesOther_csv <- fs::dir_ls("./data/raw/WinSituData/Exported Data", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
filesOther_csv <- filesOther_csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
filesOther_csv <- filesOther_csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

filesOther_csv %>%
  distinct(file_name) %>% datatable()
```

In this directory alone, there are `r filesOther_csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. 

```{r}
## table of distinct file names
filesOther_csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 'Exported Data' folder", subtitle = "A bunch of S$#T. There are files from Sara Bisbing, for God's sake")

```

```{r}
## there are baromerged files. These have different columns

## extract the baromerged
filesOther_csv.baromrg <- filesOther_csv %>% 
  filter(str_detect(file_name, "BaroMerge"))

filesOther_csv.baromrg %>% 
  distinct(file_name) %>% 
  gt()



```

```{r}
## exclude the baromerged
filesOther_csv <- filesOther_csv %>% 
  filter(!str_detect(file_name, "BaroMerge"))         
## add some header info
filesOther_csv <- filesOther_csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

## Munge: extract out info from file header
filesOther_csv <- filesOther_csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() 
```

```{r, eval=FALSE}
filesOther_csv %>%
  pluck(5,3) %>% 
  # head() %>% 
  View()

```

> Attribute with the serial number and site from the header. Note: the site is as exists in the logger. Does not match format used elswhere, so more munging...

```{r}
## extract out the actual data
logger_rawOther <- filesOther_csv %>%
  unnest(import)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header
logger_rawOther <- logger_rawOther %>%
  select(file_name,date_time, psi, temp_c, serial_number) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))

```

> Distill to daily time step

```{r}
## create daily
logger_rawOth.dly <- logger_rawOther %>% 
  group_by(file_name,date, serial_number) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  gt() %>% 
  tab_header(title = "Most files in this batch do not have any 2016+ data")

```

Most this seems useless. It should be in the DCC if max year <=2015

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  filter(max.yr >= 2016) %>% 
  gt() %>% 
  tab_header(title = "Only three files in this batch have any 2016+ data")

```



```{r}

rawOth.16.logger <- logger_rawOth.dly %>% 
  filter(psi.dly.mean < 100) %>%
  filter(date > '2016-01-01') 

```

```{r, eval=FALSE}
rawOth.16.logger %>% 
  distinct(file_name)

```

# >>
```{r}
# rawOth.16.logger %>% 
#   names()

rawOth.16.logger <- left_join(rawOth.16.logger, tow.baro.dly) 

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(site2 = case_when(grepl("Elk5", file_name, ignore.case = TRUE) ~ "elk5-obs",
                                     TRUE ~ file_name))

rawOth.16.logger <- rawOth.16.logger %>%
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) %>% 
  mutate(yr = as.integer(year(date)))

```

```{r}
## bind with the 2018 and 2019 scrape
logger.all <- bind_rows(rawOth.16.logger, logger_raw2018_19.dly.baro)

# logger.all %>% 
#   mutate(yr = as.integer(year(date))) %>% 
#   visdat::vis_dat(warn_large_data = FALSE)

```

All that crap to net 500 records in 2016 for one site!

## QA plots
```{r}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~yr, scales = "free")

```

```{r}
logger.all %>%
  group_by(serial_number, site, plot, yr) %>% 
  tally() %>% 
  ggplot(aes(yr, serial_number)) +
  geom_tile(aes(fill = n), color = "white") +
  facet_wrap(~site, scales = "free", ncol=2)

```

```{r}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~yr, scales = "free", ncol=2) +
  theme_minimal() +
  labs(caption = "combined scrape of all raw files")

```

```{r, echo=FALSE, eval=FALSE}
# see https://aosmith.rbind.io/2018/08/20/automating-exploratory-plots/
scatter_fun = function(x, y) {
     ggplot(dat, aes(x = .data[[x]], y = .data[[y]]) ) +
          geom_point() +
          geom_smooth(method = "loess", se = FALSE, color = "grey74") +
          theme_bw() +
          labs(x = x,
               y = y)
}

## line function
line_fun = function(x, y) {
     ggplot(dat, aes(x = .data[[x]], y = .data[[y]]) ) +
          geom_line() +
          theme_minimal() +
          labs(x = x,
               y = y)
}


elev_plots = map(expl, ~scatter_fun(.x, "elev") )

```



## Export Archivable Version of Cleaned Data for Further Analysis