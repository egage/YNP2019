---
title: "Well Data - Data Loggers"
output:
  html_document: 
    fig_caption: yes
    fig_height: 8
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
# opts_knit$set(root.dir=normalizePath('../')) # required if Rmd is nested below the project directory

```

```{r, eval=TRUE, echo = TRUE}
library(dplyr)
files.csv <- fs::dir_ls("./data/raw/WinSituData", recurse = TRUE, glob = "*.csv") %>% 
  length()
```

**Updated: `r format(Sys.time(), '%Y %B %d')`**

## Introduction

This document and code provides scrape data from the scattered well logger files, combines into a single analyzable file. In addition to the screening and merging of over `r files.csv`  files, the code conducts basic exploratory analyses of well logger data, primarily for further QA/QC.

The inherited state of organization for the data logger files was a mess. There are dozens of partially overlapping and inconsistently named files to munge. The DCC data (the data that was added to the DCC library repository) are assumed to be reliable. 
The code crapes data from raw logger files discovered while searching for data in the different shared dropbox/drive folders and DK hard drives. 

DCC were processed to yield relative water table elevations by D. Kotter, while newer files need correction for stickup/hang depth. These data are presumed to exist somewhere and are needed to evaluate possible errors in barometric corrections used from the sole barotroll (Tower) found for the post-DCC period. As indicated, these data are raw and need connection to field measurements (e.g., hang height, well stickup). Combined data retain file_name and serial number as attributes to aid in the necessary detective work using field notes.

Loggers vary in their start and logging interval, so to synchronize, data were reduced to a daily step for export.

```{r,echo=FALSE}
# library(here)
# here()
# install.packages("bindrcpp")
suppressPackageStartupMessages(library(tidyverse))
library(fs)
suppressPackageStartupMessages(library(sf))
# library(raster)
suppressPackageStartupMessages(library(janitor))
suppressPackageStartupMessages(library(readxl))
# library(glue)
suppressPackageStartupMessages(library(mapview))
# library(ggmap)
# library(ggrepel)
suppressPackageStartupMessages(library(viridis))
library(ggExtra)
library(DT)
library(kableExtra)
suppressPackageStartupMessages(library(lubridate))
library(anytime)
suppressPackageStartupMessages(library(compare))
suppressPackageStartupMessages(library(skimr)) ## some useful functions
suppressPackageStartupMessages(library(dataMaid))
library(gt)
```


# 2012-2015 (DCC Collection)

### Data import and initial inspection 
```{r}
# import of tsv 
dcc.raw <- readr::read_tsv("data/NSF_DataArchive20180208/Well_Logger_Data_2012-2015/Yell_Well_Data_Logger2012_2015.txt")

dcc <- dcc.raw %>% 
  clean_names() %>% 
  tibble::rownames_to_column(var = "RecordID") 

```

There are various issues apparent on inspection.  For example:

> There are corrupt data in the DCC file. No useable time stamps -- everything is characterer in date format.     
> "date_and_time" is not formatted as dttm, just a simple date. It's redundant with 'date'
> The 'time' column is not, in fact, time data.
May wish to find the orginal logger data. This may work for daily aggregated data. Otherwise, not sure what to do with the 6 per-day records. 
> The 'second's column ranges from `r min(dcc$seconds)` to `r max(dcc$seconds)`. I guess this is a cumulative time elapsed, but from when to when? I'm still of the mind that daily aggregates are the way to go forward...

```{r}
## data type changes
dcc <- dcc %>% 
  mutate(date = mdy(date))

## change case to ease merging with non-DCC data
dcc <- dcc %>% 
  mutate(site = tolower(site)) %>% 
  mutate(wat_id2 = tolower(wat_id2)) %>% 
  mutate(year = as.integer(year)) %>% 
  rename(serial_number = serial_num)

```

```{r}
## make lower case
dcc <- dcc %>% 
  mutate_if(.predicate = is.character,.funs = tolower)

## modify "site" to be consistent with usage elsewhere
dcc <- dcc %>%
  rename(site.dcc = site) %>% 
  mutate(site = case_when(
    grepl("eb1", site.dcc) ~ "eb1",
    grepl("eb2", site.dcc) ~ "eb2",
    grepl("elk1", site.dcc) ~ "elk1",
    grepl("elk4", site.dcc) ~ "elk4",
    grepl("elkdx", site.dcc) ~ "elk",
    grepl("elkcc", site.dcc) ~ "elk",
    grepl("wbcc", site.dcc) ~ "wb",
    grepl("wbdx", site.dcc) ~ "wb",
    TRUE ~ site.dcc))

## add "plot" consistent with usage elsewhere
dcc <- dcc %>% 
  mutate(plot = case_when(
    grepl("cc", site.dcc) ~ "cc",
    grepl("dx", site.dcc) ~ "dx",
    grepl("dc", site.dcc) ~ "dc",
    grepl("cx", site.dcc) ~ "cx",
    TRUE ~ "obs")) %>% 
  mutate(site2 = paste0(site,"-",plot)) %>% 
  select(-site.dcc)
```

```{r}
# create a lu
lu.serialNum.watId2 <- dcc %>%
  distinct(serial_number, wat_id2)

```


```{r, eval=FALSE}
visdat::vis_dat(dcc, warn_large_data = FALSE)

dcc %>% 
  distinct(time)
```

```{r}
## what are the distinct wells?
# dcc %>% 
#   distinct(site_logger) %>% 
#   gt() %>% 
#   tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

# note there are 19 distinct "site_logger" values and 19 distinct "serial_num"
dcc %>% 
  distinct(site, serial_number) %>% 
  gt() %>% 
  tab_header(title = "DCC logger data", subtitle = "Distinct 'site_logger'")

```


```{r}
### summarize at a daily time step
dcc.dly.mean <- dcc %>% 
  group_by(site, plot, site2, wat_id2, serial_number, date) %>% 
  summarise(mean.dly.abslevel = mean(abslevel,na.rm = TRUE), mean.water_column_m = mean(water_column_m, na.rm = TRUE), psi_logger = mean(press_psi, na.rm=TRUE),psi_baro = mean(bar_press_psi, na.rm=TRUE))

dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(plot = case_when(str_detect(site2, "cc") ~ "cc",
                          str_detect(site2, "dc") ~ "dc",
                          str_detect(site2, "dx") ~ "dx",
                          str_detect(site2, "cx") ~ "cx",
                          TRUE ~ "obs"))

## add in year and mob=nth columns
dcc.dly.mean <- dcc.dly.mean %>% 
  mutate(yr = lubridate::year(date)) %>% 
  mutate(month = lubridate::month(date)) %>% 
  mutate(doy = lubridate::yday(date)) %>% 
  mutate(doy.d = format(date, format="%m-%d"))

```

```{r, eval=FALSE}
## some plots for QA
dcc.dly.mean %>% 
  mutate(mean.water_column_m = mean.water_column_m*-1) %>%
  # distinct(plot)
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.water_column_m)) +
  geom_line(aes(color=serial_number)) +
  geom_hline(yintercept = 0, color = "red", lty = "dashed") +
  labs(title = "DCC: Mean daily 'water_column_m' values") +
  facet_wrap(~site, ncol=2) +
  theme_minimal() +
  labs(x = "DOY", y = "mean.dly.abslevel", caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

```

```{r}
dcc.dly.mean %>% 
  filter(plot != "obs") %>% 
  ggplot(aes(x=date,y=mean.dly.abslevel)) +
  geom_line(color="blue") +
  labs(title = "DCC: Mean daily 'abslevel' values") +
  facet_wrap(~serial_number, scales = "free_y") +
  theme_minimal() +
  labs(x = "Date", y = "Water table elevation (m)",caption = "mean.dly.abslevel = mean(abslevel,na.rm = TRUE)")

# ggsave("waterlevel_loggers_exp.png", width = 8.5, height = 6)
```


```{r, fig.height=15}
## plot daily traces
dcc.dly.mean %>% # names()
  filter(plot != "obs") %>%
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  # ggplot(aes(x=doy,y=mean.water_column_m)) +
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  # labs(title = "Mean daily 'water_column_m' values") +
  facet_wrap(~wat_id2, scale = 'free_y', ncol = 3) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```

DCC daily traces

```{r, fig.height=15}
## plot daily traces

dcc.dly.mean %>%
  filter(plot != "obs") %>%
  filter(month > 2 & month < 10) %>% 
  mutate(yr = as.factor(yr)) %>% 
  ggplot(aes(x=doy,y=mean.dly.abslevel)) +
  geom_line(aes(color=yr)) +
  labs(x = "Day of year", y = "Water table elevation (m)") +
  facet_wrap(~site2, scale = 'free_y', ncol = 2) +
  theme_minimal()
# ggsave("waterlevel_loggers_exp_alt2.png", width = 8.5, height = 6)

```


```{r, eval=FALSE}
### Export munged DCC data
dcc.dly.mean %>% 
  write_csv("./data/processed/data_loggers/dcc_logger_20201205.csv")

```

# Non-DCC 2016-2019 files

General strategy is to work backwards from 2019 as there innumerable 'appended' files. Maybe these capture data from earlier periods?


```{r, echo=FALSE, eval=FALSE}
## File inventory:  *.wsl
## find WSL files
files_all_wsl <- fs::dir_ls("./data/raw/WinSituData", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

files_all_wsl %>% 
  select(-path_full) %>% 
  datatable(caption = "All *.wsl files")
#  A total shit show, as expected.
```

There _many_ files. Some don't even seem to be from YELL (e.g. files from Sara Bisbing's AK work).

## Tower baro data

In this list is the Tower baro data. Pull this out. I believe it should be used for correcting all the different site data? There seem to be no other baro files....

Source file: TowerBarow_2011+_2019-09-10_11-01-36-258.csv
Based on the date range, this may extend to the full time series?
```{r}
tow.baro <- read_csv("data/raw/WinSituData/Logger_Data_2019/TowerBarow_2011+_2019-09-10_11-01-36-258.csv", skip=84) %>% 
  clean_names() %>% 
  select(-x5) %>%
  mutate(date = anytime::anydate(date_and_time))

## the start of the trace looks a little dodgy, trim
tow.baro <- tow.baro %>% 
  filter(date > 2011-10-02)

## extract date and calc mean daily pressure (entered in mm Hg) and temp
tow.baro.dly <- tow.baro %>% 
  group_by(date) %>% 
  dplyr::summarize(mmHg_mean = mean(pressure_mm_hg, na.rm = TRUE), temp_c_mean = mean(temperature_c)) %>% 
  mutate(yr = year(date))

# convert the mm Hg to psi (units the loggers are in)
tow.baro.dly <- tow.baro.dly %>% 
  select(-yr) %>%
  mutate(baroTow_psi = 0.0193368*mmHg_mean) %>% 
  rename(baroTow_tempC = temp_c_mean) %>% 
  rename(baroTow_mmHg = mmHg_mean)

```

```{r}
## plot baro data
tow.baro.dly %>% 
  # filter(mmHg.mean < 620) %>% 
  ggplot(aes(date, baroTow_psi)) +
  geom_line(color = 'blue') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily psi", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

tow.baro.dly %>% 
  ggplot(aes(date, baroTow_tempC)) +
  geom_line(color = 'red') +
  theme_minimal() +
  labs(title = "Tower Baro daily trace", subtitle = "mean daily temp_c", caption  = "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

Temps below -30 C seem highly questionable...


## 2019 Data

### Data import and initial inspection 

```{r, echo=FALSE, eval=FALSE}
## find WSL files
files2019wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))
```

```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2019csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2019", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
files2019csv <- files2019csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2019csv <- files2019csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

```

For for 2019, there are `r files2019csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. These appear to mainly be 
raw export files from WinSitu. Pending on the particular notes entered in the logger, data starts at a different row in the *.csv, complicating parsing. Data are uncorrected. The only baro data seems to be from Tower and is used to process the data.

```{r}
## table of distinct file names
files2019csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2019 directory")

# Pull out what appears to be the only baro file
```

```{r}
## exclude the baro from the remaining logger files
files2019csv <- files2019csv %>% 
  filter(file_name != "TowerBarow_2011+_2019-09-10_11-01-36-258.csv")

```

```{r}
## all of the csv exports lack depth; uncorrected.

## add some header info
files2019csv <- files2019csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

## Munge: extract out info from file header
files2019csv <- files2019csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  mutate(site_hdr = site)

```

Note: 'site_hdr' refers to the 'site' attribute recorded in the export file (i.e., it was entered in the logger).

```{r, eval=FALSE}
files2019csv %>%
  pluck(5,3) %>% 
  # head() %>% 
  View()

```

> Attribute with the serial number and site from the header. Note: the site is as exists in the logger. Does not match format used elswhere, so more munging...

```{r}
## extract out the actual data
logger_raw2019 <- files2019csv %>%
  unnest(import)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header
logger_raw2019 <- logger_raw2019 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```

> Distill to daily time step

```{r}
## create daily
logger_raw2019.dly <- logger_raw2019 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2019.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```

```{r}
## join in the baro

logger_raw2019.dly.baro <- left_join(logger_raw2019.dly, tow.baro.dly, by = "date") 

```

```{r, eval=FALSE}
logger_raw2019.dly.baro %>% 
  visdat::vis_dat()

logger_raw2019.dly.baro %>% 
  View()

```


## 2018 Data 

### Data import and initial inspection 


Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
-	**Data file labeled Slough Creek is WBCC.** Pressure transducer was re-labeled for Slough Creek in 2017 but never deployed. Redeployed in spring to WBCC and not renamed. 
-	Crystal well was removed in either May or June 2018 and replaced in July 2018. Check timing of removal in May/June. Logger was likely removed from the well, not paused, and returned in July after a reset. 
-	All data need to be post-processed. All stick-up, total depth, and hanging distances are recorded in the well measurement field note books, which will eventually be copied to a spreadsheet.  


```{r}
# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
files2018csv <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.csv")

## find WSL files
files2018wsl <- fs::dir_ls("./data/raw/WinSituData/Logger_Data_2018", recurse = TRUE, glob = "*.wsl") %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name) %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full))

## make a tibble, rename
files2018csv <- files2018csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
files2018csv <- files2018csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

# files2018csv %>% 
#   distinct(file_name) %>% datatable()
```

For for 2018 alone, there are `r files2018csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconstienly structured. 

```{r}
## table of distinct file names
files2018csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 2018 directory")

```

```{r}
## all of the csv exports lack depth; uncorrected.
files2018csv <- files2018csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 40))

## Munge: extract out info from file header
files2018csv <- files2018csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  # distinct(hdr_var)
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() %>% 
  rename(site_hdr = site) 

## remove baro
files2018csv <- files2018csv %>%
  filter(file_name != "2011+_2018-08-22_08-59-44-597.csv") # remove the tower baro data. I've previously pulled that out (and it's in mm Hg)

```

```{r, eval=FALSE}
files2018csv %>%
  pluck(5,2) %>% 
  head() %>% View()

files2018csv %>% 
  pluck(4,2) %>% datatable()

## get names
files2018csv %>% 
  mutate(names = map(import, names)) %>% 
  unnest(names) %>% 
  datatable()

```

```{r, eval=FALSE}
## explore parsing
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv", skip = 69)
t
t <- read_csv("./data/raw/WinSituData/Logger_Data_2018/Exported Data/Exported_20181024/EB2_dc_2012+_Append_2018-08-07_15-00-09-695.csv")
t
```

```{r}
## extract out the actual data
logger_raw2018 <- files2018csv %>%
  unnest(import)

# logger_raw2018 %>% 
#   distinct(file_name)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header. Take a long time to process...
logger_raw2018 <- logger_raw2018 %>%
  select(file_name,date_time, psi, temp_c, serial_number, site_hdr) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))
```


> Distill to daily time step

```{r}
## create daily
logger_raw2018.dly <- logger_raw2018 %>% 
  group_by(file_name,date, serial_number, site_hdr) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r, eval=FALSE}
## qa
logger_raw2018.dly %>% 
  ggplot(aes(date,psi.dly.mean)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~site_hdr)
  
```


```{r}
## join in the baro

logger_raw2018.dly.baro <- left_join(logger_raw2018.dly, tow.baro.dly, by = "date") 

```

```{r, eval=FALSE}
logger_raw2018.dly.baro %>% 
  visdat::vis_dat()

logger_raw2018.dly.baro %>% 
  View()

```

```{r}
# combine and distill down the 2018 and 2019 batches
logger_raw2018_19.dly.baro <- bind_rows(logger_raw2018.dly.baro, logger_raw2019.dly.baro) %>% 
  distinct()

## add yr
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(yr = as.integer(year(date)))
```

```{r}
## qa
# logger_raw2018_19.dly.baro %>% 
#   head()

logger_raw2018_19.dly.baro %>% 
  tabyl(site_hdr, yr) %>% 
  datatable(caption = "Count of records by yr and site_hdr")

logger_raw2018_19.dly.baro %>% 
  tabyl(serial_number, yr) %>% 
  datatable(caption = "Count of records by yr and serial number")

```

```{r}
## munge site
logger_raw2018_19.dly.baro %>% 
  distinct(site_hdr)

## do my best to guess site and plot from the site_hdr field derived from logger header
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(site2 = case_when(site_hdr == "WestBlacktail_exp dx" ~ "wb-dx",
                           site_hdr == "Crystal_Well" ~ "crystal-obs",
                           site_hdr == "Crystal3" ~ "crystal-obs",
                           site_hdr == "EastBlacktail1_exp cc" ~ "eb1-cc",
                           site_hdr == "EastBlacktail1_exp dx" ~ "eb1-dx",
                           site_hdr == "EastBlacktail2_obs" ~ "eb2-obs",
                           site_hdr == "EastBlacktail2_exp cc" ~ "eb2-cc",
                           site_hdr == "EastBlacktail2_exp dc" ~ "eb2-dc",
                           site_hdr == "EastBlacktail2_exp dx" ~ "eb2-dx",
                           site_hdr == "Elk1_obs" ~ "elk1-obs",
                           site_hdr == "Elk4_obs" ~ "elk4-obs",
                           site_hdr == "Elk5" ~ "elk5-obs",
                           site_hdr == "Elk_exp cc" ~ "elk-cc",
                           site_hdr == "Elk_exp dx" ~ "elk-dx",
                           site_hdr == "LowerBlacktail2_obs" ~ "lb2-obs",
                           site_hdr == "LB4_WELL" ~ "lb4-obs",
                           site_hdr == "Oxbow_obs" ~ "oxbow-obs",
                           site_hdr == "Slough" ~ "wb-cc",
                           site_hdr == "WB4" ~ "wb4-obs",
                           site_hdr == "LostCreek_obs" ~ "lostc-obs",
                           TRUE ~ site_hdr)) %>% 
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

# Notes on data collected during the 2018 field season: Lewis Messner - 10/24/2018
# -	**Data file labeled Slough Creek is WBCC.
```

```{r}
logger_raw2018_19.dly.baro %>% 
  select(serial_number,site_hdr,site2,site,plot) %>% 
  distinct() %>% 
  gt() %>% 
  tab_header(title = "site info attributed from logger site (site_hdr)", subtitle = "LEWIS please check")

## write to csv to share with LEWIS
# logger_raw2018_19.dly.baro %>% 
#   select(serial_number,site_hdr,site2,site,plot) %>% 
#   distinct() %>%
#   write_csv("./data/processed/logger_site_attribution_4QC.csv")

```

```{r}
## first attempt at baro correction
logger_raw2018_19.dly.baro <- logger_raw2018_19.dly.baro %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) 

```

```{r}
logger_raw2018_19.dly.baro %>% 
  summarytools::descr(head_cm) %>% 
  summarytools::tb()

logger_raw2018_19.dly.baro %>% 
  ggplot(aes(date,head_cm)) +
  geom_line(aes(color = plot)) +
  facet_wrap(~site) +
  labs(title = "QA plot", subtitle = "logger_psi - baro_psi converted to cm of head")

```

I don't know where missing site data should be. Also, info on the specific well id and location is lacking, so not able to compare to manual well measurements. No notes on stick up or hang depth needed to further process the data. 

## Misc other files 

There are several additional directories of files. Scraping these for any data >2016. 
Presume earlier data folded up in the DCC?

### Data import and initial inspection 

```{r}
# the lack of data management on this project is ridiculous.

# read directory for all exported logger files 
# there's a crazy number of inconsistently entered fields. Nothing is clear.
filesOther_csv <- fs::dir_ls("./data/raw/WinSituData/Exported Data", recurse = TRUE, glob = "*.csv")

## make a tibble, rename
filesOther_csv <- filesOther_csv %>% 
  enframe() %>% 
  rename(path_full = value) %>% 
  select(-name)

## add naked file name and naked path
filesOther_csv <- filesOther_csv %>% 
  mutate(path = fs::path_dir(path_full)) %>% 
  mutate(file_name = fs::path_file(path_full)) 

filesOther_csv %>%
  distinct(file_name) %>% datatable()
```

In this directory alone, there are `r filesOther_csv %>% 
  distinct(file_name) %>% nrow()` distinct csv files in the logger directory. 
Examination of these files reveals that they're inconsistently structured. 

```{r}
## table of distinct file names
filesOther_csv %>% 
  distinct(file_name) %>%
  gt::gt() %>% 
  gt::tab_header(title = "Distinct csv files in the 'Exported Data' folder", subtitle = "A bunch of S$#T. All older than DCC. There are even files from Sara Bisbing's AK project.")

```

```{r}
## there are baromerged files. These have different columns

## extract the baromerged
filesOther_csv.baromrg <- filesOther_csv %>% 
  filter(str_detect(file_name, "BaroMerge"))


```

```{r}
## exclude the baromerged
filesOther_csv <- filesOther_csv %>% 
  filter(!str_detect(file_name, "BaroMerge"))         
## add some header info
filesOther_csv <- filesOther_csv %>% 
  mutate(import = map(.x = path_full, .f = read_csv, skip=100, col_names = c("date_time","seconds","psi","temp_c"))) %>% 
  mutate(header = map(path_full, read_csv, skip = 0,
  n_max = 10))

## Munge: extract out info from file header
filesOther_csv <- filesOther_csv %>% 
  mutate(hdr_info = map(path_full, read_csv, skip =13,
  n_max = 13, col_names = c("hdr_var","hdr_val"))) %>% 
  unnest(hdr_info) %>% 
  filter(!is.na(hdr_val) & !is.na(hdr_var)) %>%
  filter(hdr_var == "Site" | hdr_var == "Serial Number") %>% 
  pivot_wider(names_from = hdr_var, values_from = hdr_val) %>% 
  clean_names() 
```

> Attribute with the serial number from the header. 

```{r}
## extract out the actual data
logger_rawOther <- filesOther_csv %>%
  unnest(import)

```

```{r, cache=TRUE}
# select and attribute with the serial number and site from the header
logger_rawOther <- logger_rawOther %>%
  select(file_name,date_time, psi, temp_c, serial_number) %>% 
  distinct() %>% 
  mutate(date_time = anytime::anytime(date_time)) %>% 
  mutate(date = date(date_time)) %>% 
  mutate(yr = year(date_time))

```

> Distill to daily time step

```{r}
## create daily
logger_rawOth.dly <- logger_rawOther %>% 
  group_by(file_name,date, serial_number) %>% 
  dplyr::summarise(psi.dly.mean = mean(psi, na.rm=TRUE), tempC.dly.mean = mean(temp_c, na.rm=TRUE)) %>%
  ungroup() %>% 
  distinct()

```

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  gt() %>% 
  tab_header(title = "Most files in this batch do not have any 2016+ data")

```

>Most this seems useless. It should be in the DCC if max year <=2015

```{r}
logger_rawOth.dly %>% 
  mutate(yr = as.integer(year(date))) %>% 
  group_by(file_name) %>% 
  summarise(min.yr = min(yr), max.yr = max(yr)) %>% 
  filter(max.yr >= 2016) %>% 
  gt() %>% 
  tab_header(title = "Only three files in this batch have any 2016+ data")

```



```{r}

rawOth.16.logger <- logger_rawOth.dly %>% 
  filter(psi.dly.mean < 100) %>% # baro data in mmHg filtered out here
  filter(date > '2016-01-01')

```

```{r, eval=FALSE}
rawOth.16.logger %>% 
  distinct(file_name) %>% 
  gt() %>% 
  tab_header(title = "The only new data produced from this directory scrape")

```


```{r}
# rawOth.16.logger %>% 
#   names()

rawOth.16.logger <- left_join(rawOth.16.logger, tow.baro.dly) 

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(site2 = case_when(grepl("Elk5", file_name, ignore.case = TRUE) ~ "elk5-obs",
                                     TRUE ~ file_name))

rawOth.16.logger <- rawOth.16.logger %>%
  separate(site2, c("site","plot"),sep = "-", remove = FALSE)

rawOth.16.logger <- rawOth.16.logger %>% 
  mutate(psi.dif = psi.dly.mean-baroTow_psi) %>%
  mutate(head_cm = psi.dif*0.70324961490205*100) %>% 
  mutate(yr = as.integer(year(date)))

```

```{r}
## bind with the 2018 and 2019 scrape
logger.all <- bind_rows(rawOth.16.logger, logger_raw2018_19.dly.baro)

# logger.all %>% 
#   mutate(yr = as.integer(year(date))) %>% 
#   visdat::vis_dat(warn_large_data = FALSE)

```

All that crap to net ~500 records in 2016 for one site!

> Join in wat_id2 to the non-DCC scrape 
> ASSUMPTION: serial number stay with wells. The well number associated with a given serial number in the DCC is used to attribute the non-DCC scrape. 

```{r}
## Filter to 2016 +
logger.all <- logger.all %>%
  filter(yr >=2016) 
```


```{r}
## join in wat_id2 to the non-DCC scrape 
lu.serialNum.watId2 <- lu.serialNum.watId2 %>% 
  mutate(serial_number = as.character(serial_number))

logger.all <- dplyr::left_join(logger.all,lu.serialNum.watId2, by="serial_number")

```

There are multiple files with serial numbers not present in the DCC. **Are these new loggers?**


```{r}
## there are data not joining to dcc on serial number
logger.all %>% 
  filter(is.na(wat_id2)) %>%
  distinct(file_name) %>% 
  gt() %>% 
  tab_header(title = "Files lacking coresponding serial number in DCC")

```

```{r}
## Filter to 2016 +
logger.all <- logger.all %>%
  filter(yr >=2016) 

## prep for row bind with DCC

dcc.dly.mean <- dcc.dly.mean %>% 
  ungroup() %>% 
  mutate(file_name = "DCC") %>% 
  mutate(serial_number = as.character(serial_number)) %>% 
  mutate(head_cm = mean.water_column_m*100) %>% 
  mutate(serial_number = as.character(serial_number)) %>% 
  select(-c(month, doy, doy.d, mean.water_column_m)) 
logger.all <- logger.all %>% 
  select(-c(baroTow_mmHg,baroTow_tempC)) %>% 
  rename(psi_logger = psi.dly.mean) %>% 
  rename(psi_baro = baroTow_psi) %>% 
  select(-c(site_hdr))

logger.all %>%
  glimpse()

# compare_df_cols_same(logger.all, dcc.dly.mean)

```

```{r}
### BIND DCC
logger.all <- logger.all %>% 
  bind_rows(.,dcc.dly.mean) %>% 
  distinct()
```


```{r}
## add in dates
logger.all <- logger.all %>% 
  mutate(yr = lubridate::year(date)) %>% 
  mutate(month = lubridate::month(date)) %>% 
  mutate(doy = lubridate::yday(date)) %>% 
  mutate(doy.d = format(date, format="%m-%d"))

```


## QA plots

```{r}
logger.all %>% 
  visdat::vis_dat(warn_large_data = FALSE) +
  labs(caption = "Combined DCC and other logger data")
```


```{r, fig.width=7.5, fig.height=8}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")

```

> Interactive time series plot - Exp sites only

```{r, fig.width=7.5, fig.height=8}
qa.pl.exp <- logger.all %>%
  filter(plot != "obs") %>% 
  ggplot(aes(date, head_cm)) +
  geom_point(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")
plotly::ggplotly(qa.pl.exp)
```

> Interactive time series plot - Obs sites only

```{r, fig.width=7.5, fig.height=8}
qa.pl.obs <- logger.all %>%
  filter(plot == "obs") %>% 
  ggplot(aes(date, head_cm)) +
  geom_point(aes(color = serial_number)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~yr, scales = "free", ncol = 2) +
  labs(x="", y="Head (cm)", caption = "logger.all")
plotly::ggplotly(qa.pl.obs)
```

```{r, fig.width=7.5, fig.height=10}
logger.all %>%
  mutate(doy = yday(date)) %>%
  mutate(yr = as.factor(yr)) %>% 
  ggplot(aes(doy, head_cm)) +
  geom_line(aes(color = yr)) +
  viridis::scale_color_viridis(discrete=TRUE) +
  theme_minimal() +
  facet_wrap(~serial_number, scales = "free", ncol = 2) +
  labs(title = "Time series -- all logger data", x="", y="Head (cm)", caption = "logger.all+DCC")

```


```{r, fig.width=7, fig.height=6.5}
logger.all %>%
  group_by(serial_number, site, plot, yr) %>% 
  tally() %>% 
  ggplot(aes(yr, serial_number)) +
  theme_minimal() +
  geom_tile(aes(fill = n), color = "white") +
  facet_wrap(~site, scales = "free", ncol=2) +
  labs(fill = "n records", y = "Serial number", caption = "logger.all+DCC")

```

```{r}
logger.all %>%
  ggplot(aes(date, head_cm)) +
  geom_line(aes(color = serial_number)) +
  facet_wrap(~yr, scales = "free", ncol=2) +
  theme_minimal() +
  labs(caption = "combined scrape of all raw files + DCC")

```


There are gaps in data :)


```{r, eval=FALSE}
## Export for Further Analysis
logger.all %>% 
  write_csv("./data/processed/data_loggers/combined_dataloggers_20201205.csv")

```

## Next steps

> Find information (if it exists) on hang depths, stickups?

> Where are loggers _supposed_ to be?

 
